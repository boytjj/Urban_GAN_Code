{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Set GPU\n",
    "import os\n",
    "import random\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' #Set GPU number"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import make_grid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not os.path.exists('./checkpoint'):\n",
    "  os.mkdir('./checkpoint')\n",
    "if not os.path.exists('./dataset'):\n",
    "  os.mkdir('./dataset')\n",
    "if not os.path.exists('./img'):\n",
    "  os.mkdir('./img')\n",
    "if not os.path.exists('./img/real'):\n",
    "  os.mkdir('./img/real')\n",
    "if not os.path.exists('./img/fake'):\n",
    "  os.mkdir('./img/fake')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visualize the first image from the torch tensor\n",
    "def vis_image(image):\n",
    "    plt.imshow(image[0].detach().cpu().numpy(),cmap='gray')\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_gif(training_progress_images, images):\n",
    "    '''\n",
    "        training_progress_images: list of training images generated each iteration\n",
    "        images: image that is generated in this iteration\n",
    "    '''\n",
    "    img_grid = make_grid(images.data)\n",
    "    img_grid = np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0))\n",
    "    img_grid = 255. * img_grid \n",
    "    img_grid = img_grid.astype(np.uint8)\n",
    "    training_progress_images.append(img_grid)\n",
    "    imageio.mimsave('./img/training_progress.gif', training_progress_images)\n",
    "    return training_progress_images"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visualize gif file\n",
    "def plot_gif(training_progress_images, plot_length=10):\n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    total_len = len(training_progress_images)\n",
    "    for i in range(plot_length):\n",
    "        im = plt.imshow(training_progress_images[int(total_len/plot_length)*i])\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_image_list(dataset, real):\n",
    "  if real:\n",
    "    base_path = './img/real'\n",
    "  else:\n",
    "    base_path = './img/fake'\n",
    "\n",
    "  dataset_path = []\n",
    "\n",
    "  for i in range(len(dataset)):\n",
    "    save_path = f'{base_path}/image_{i}.png'\n",
    "    dataset_path.append(save_path)\n",
    "    vutils.save_image(dataset[i], save_path)\n",
    "\n",
    "  return base_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataroot = './training_blk/'\n",
    "from PIL import Image\n",
    "def custom_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGBA')\n",
    "\n",
    "#dataset = dset.ImageFolder(root = dataroot, loader = custom_loader)\n",
    "#dataset = dset.ImageFolder(root = dataroot, transform = transforms.Compose([transforms.Resize(64), transforms.CenterCrop(64), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5,0.5))]))\n",
    "dataset = dset.ImageFolder(root = dataroot, transform = transforms.Compose([transforms.Resize(64), transforms.CenterCrop(64), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5,0.5, 0.5))]), loader = custom_loader)\n",
    "#dataset = torch.utils.data.TensorDataset(tensor_a, transform = transforms.Compose([transforms.Resize(64), transforms.CenterCrop(64), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5,0.5))]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 128, shuffle = True, num_workers = 2)\n",
    "device = torch.device('cuda:0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    #print(i, \"  Data: \", data)\n",
    "    print(data[0].shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Intialize convolutional, convolutional-transpose from a Normal distribution (0, 0.02) and batch normalization layers to meet this criteria\n",
    "def weights_init(m):\n",
    "  classname = m.__class__.__name__\n",
    "  if classname.find('Conv') != -1:\n",
    "    nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "  elif classname.find('BatchNorm') != -1:\n",
    "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "    nn.init.constant_(m.bias.data, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Num of channels in training image (In this case, RGB-3)\n",
    "nc = 4\n",
    "# Size of z latent vector (Size of generator input)\n",
    "nz = 100\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "num_epochs = 150\n",
    "lr = 0.0002\n",
    "# Beta1 hyperparam for Adam potimizers\n",
    "beta1 = 0.5\n",
    "# Number of GPUs available.\n",
    "ngpu = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "######################---------Genrator----------#######################\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self, ngpu):\n",
    "    super(Generator, self).__init__()\n",
    "    self.ngpu = ngpu\n",
    "    self.main = nn.Sequential(\n",
    "        #Input latent Z\n",
    "        nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias = False),\n",
    "        nn.BatchNorm2d(ngf*8),\n",
    "        nn.ReLU(True),\n",
    "        #1st state (ngf*8 *4 *4)\n",
    "        nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*4),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 2),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf),\n",
    "        nn.ReLU(True),\n",
    "        #state size(ngf) X 32 X 32\n",
    "        nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias = False),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "  def forward(self, input):\n",
    "    output = self.main(input)\n",
    "    return output\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "######################---------Genrator----------#######################\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self, ngpu):\n",
    "    super(Generator, self).__init__()\n",
    "    self.ngpu = ngpu\n",
    "    self.main = nn.Sequential(\n",
    "        #Input latent Z\n",
    "        nn.ConvTranspose2d(nz, ngf*16, 4, 1, 0, bias = False),\n",
    "        nn.BatchNorm2d(ngf*16),\n",
    "        nn.ReLU(True),\n",
    "        #0st state (ngf*16 *4 *4)\n",
    "        nn.ConvTranspose2d(ngf*16, ngf*8, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*8),\n",
    "        nn.ReLU(True),\n",
    "        #1st state (ngf*8 *8 *8)\n",
    "        nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*4),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*4) x 16 x 16\n",
    "        nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 2),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*2) x 32  x 32\n",
    "        nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf),\n",
    "        nn.ReLU(True),\n",
    "        #state size(ngf) X 64 X 64\n",
    "        nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias = False),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "  def forward(self, input):\n",
    "    output = self.main(input)\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##Create Generator\n",
    "netG = Generator(ngpu).cuda()\n",
    "\n",
    "if (device.type == 'cuda') and (ngpu>1):\n",
    "  netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print(netG)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "######################---------Discriminator----------#######################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, ngpu):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.ngpu = ngpu\n",
    "    self.main = nn.Sequential(\n",
    "        # nc * 64 * 64 is input from training set & Generator\n",
    "        nn.Conv2d(nc, ndf, 4, 2, 1, bias = False),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "         # state size. (ndf) x 32 x 32\n",
    "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*2) x 16 x 16\n",
    "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*4) x 8 x 8\n",
    "        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size , (ndf*8) *4 *4\n",
    "        nn.Conv2d(ndf*8, 1, 4, 1, 0, bias = False),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "  def forward(self, input):\n",
    "    output = self.main(input)\n",
    "    return output.view(-1, 1).squeeze(1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "######################---------Discriminator----------#######################\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, ngpu):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.ngpu = ngpu\n",
    "    self.main = nn.Sequential(\n",
    "        # nc * 128 * 128 is input from training set & Generator\n",
    "        nn.Conv2d(nc, ndf, 4, 2, 1, bias = False),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "         # state size. (ndf) x 64 x 64\n",
    "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*2) x 32 x 32\n",
    "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size. (ndf*4) x 16 x 16\n",
    "        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 8),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        #state size. (ndf*8) x 8 x8\n",
    "        nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 16),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # state size , (ndf*16) *4 *4\n",
    "        nn.Conv2d(ndf*16, 1, 4, 1, 0, bias = False),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "  def forward(self, input):\n",
    "    output = self.main(input)\n",
    "    return output.view(-1, 1).squeeze(1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Create Discriminator\n",
    "netD = Discriminator(ngpu).cuda()\n",
    "\n",
    "if (device.type == 'cuda') and (ngpu>1):\n",
    "  netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1InzR1qylS3Air4IvpS9CoamqJ0r9bqQg' -O inception.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1AtTxnuasIaSTTmI9MI7k8ugY8KJ1cw3Y' -O fid_score.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fid_score import calculate_fid_given_paths  # The code is downloaded from github"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Get Z from random noise\n",
    "fixed_noise = torch.randn(64, nz, 1, 1).cuda()\n",
    "\n",
    "#Set real or fake label\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "#Setup Optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas = (beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, 0.999))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Training Loop\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "training_progress_images_list = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Start Training!!!\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "    # Set Initiating grad\n",
    "    netD.zero_grad()\n",
    "    real_cpu = data[0].cuda()\n",
    "    b_size = real_cpu.size(0)\n",
    "    label = torch.full((b_size,), real_label, dtype=torch.float).cuda()\n",
    "    output = netD(real_cpu)\n",
    "    errD_real = criterion(output, label)\n",
    "    errD_real.backward()\n",
    "    D_x = output.mean().item()\n",
    "\n",
    " \n",
    "    #Train with fake batch\n",
    "    noise = torch.randn(b_size, nz, 1, 1).cuda()\n",
    "    #Generate Fake image by G\n",
    "    fake = netG(noise)\n",
    "    label.fill_(fake_label)\n",
    "    # Discriminate fake images by D\n",
    "    output = netD(fake.detach())\n",
    "    #Calculate loss\n",
    "    errD_fake = criterion(output,label)\n",
    "    errD_fake.backward()\n",
    "    D_G_z1 = output.mean().item()\n",
    "    \n",
    "    #Add gradient from all real & all fake batches: log(D(x)) + (1-log(D(G(z))))\n",
    "    errD = errD_real+ errD_fake\n",
    "    # Update D\n",
    "    optimizerD.step()\n",
    "\n",
    "    #Train G\n",
    "    netG.zero_grad()\n",
    "    label.fill_(real_label)\n",
    "    # Following updated result of D, G tries to learn cheating D\n",
    "    output = netD(fake)\n",
    "    errG = criterion(output, label)\n",
    "    errG.backward()\n",
    "    D_G_z2 = output.mean().item()\n",
    "    #Update G\n",
    "\n",
    "    optimizerG.step()\n",
    "\n",
    "    if i % 300 == 0:\n",
    "      print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'% (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "      fake = netG(fixed_noise)\n",
    "      training_progress_images_list = save_gif(training_progress_images_list, fake)  # Save fake image while training!\n",
    "      \n",
    "      # Check pointing for every epoch\n",
    "      torch.save(netG.state_dict(), './checkpoint/netG_epoch_%d.pth' % (epoch))\n",
    "      torch.save(netD.state_dict(), './checkpoint/netD_epoch_%d.pth' % (epoch))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataroot = './training_blk/'\n",
    "\n",
    "test_dataset = dset.ImageFolder(root= dataroot,\n",
    "                                           transform=transforms.Compose(\n",
    "                                           [transforms.Resize(64), transforms.CenterCrop(64),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5,0.5, 0.5))\n",
    "                        ]), loader = custom_loader )\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=True, num_workers=2)\n",
    "\n",
    "for i, (data, _) in enumerate(dataloader):\n",
    "    real_dataset = data\n",
    "    break\n",
    "    \n",
    "noise = torch.randn(1000, nz, 1, 1).cuda()\n",
    "fake_dataset = netG(noise)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "real_image_path_list = save_image_list(real_dataset, True)\n",
    "fake_image_path_list = save_image_list(fake_dataset, False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pcw_venv': conda)"
  },
  "interpreter": {
   "hash": "6f4fca855cbecefa3c4050351d36ae09d574a0e1f27483fb7f3c3777f1a4746a"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}